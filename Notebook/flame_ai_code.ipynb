{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This pytorch notebook has been written by Adib Bazgir and Rama chandra praneeth Madugula for Stanford Flame AI competition.this model utilises edsr baseline architecture with some modificationa and attaching SE block to the residual block to improve the efficiency\n# Other versions of this code within tensorflow framework will be developed very soon. More model architectures based on FNO, GAN, etc will be developed base on the same provided dataset.\n# This code will be promptly available on github and kaggle public notebook after the final comptetion outcome becomes released!\n\n# install the accelerate package\n\n!pip install accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-18T03:02:55.626065Z","iopub.execute_input":"2023-09-18T03:02:55.626459Z","iopub.status.idle":"2023-09-18T03:03:10.068426Z","shell.execute_reply.started":"2023-09-18T03:02:55.626423Z","shell.execute_reply":"2023-09-18T03:03:10.067269Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# import the required libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR,ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import Normalize, Compose, ToTensor\nfrom tqdm import tqdm\nimport pytorch_lightning as pl\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport datetime\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:10.071603Z","iopub.execute_input":"2023-09-18T03:03:10.072397Z","iopub.status.idle":"2023-09-18T03:03:10.080468Z","shell.execute_reply.started":"2023-09-18T03:03:10.072341Z","shell.execute_reply":"2023-09-18T03:03:10.079283Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# wipe the working directory for any output\n\n!rm -rf /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:10.082140Z","iopub.execute_input":"2023-09-18T03:03:10.083510Z","iopub.status.idle":"2023-09-18T03:03:11.069532Z","shell.execute_reply.started":"2023-09-18T03:03:10.083483Z","shell.execute_reply":"2023-09-18T03:03:11.068226Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working': Device or resource busy\n","output_type":"stream"}]},{"cell_type":"code","source":"# directories to inputs and outputs\n\nbase_path = \"/kaggle/input/2023-flame-ai-challenge/\"\nworking_dir = \"/kaggle/working/\"\ninput_path = base_path + \"dataset/\"\noutput_path = working_dir + \"outputs/\"\n\n# create directories for checkpoints and logs\nlog_dir = output_path + \"logs/\"\ncheckpoint_dir = output_path + \"ckpt/\"\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\ntrain_df = pd.read_csv(input_path + \"train.csv\")\nval_df = pd.read_csv(input_path + \"val.csv\")\ntest_df = pd.read_csv(input_path + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:11.071879Z","iopub.execute_input":"2023-09-18T03:03:11.072389Z","iopub.status.idle":"2023-09-18T03:03:11.114059Z","shell.execute_reply.started":"2023-09-18T03:03:11.072330Z","shell.execute_reply":"2023-09-18T03:03:11.113011Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# increase the rate at which output messages can be displayed\n\nfrom IPython.display import display, Javascript\n\n# Set the iopub_msg_rate_limit to a higher value\ndisplay(Javascript('''\nrequire([\"base/js/namespace\"],function(Jupyter) {\n    Jupyter.notebook.kernel.execute(\"config = get_ipython().config\");\n    Jupyter.notebook.kernel.execute(\"config.NotebookApp.iopub_msg_rate_limit = 4000.0\");\n});\n'''))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:11.117561Z","iopub.execute_input":"2023-09-18T03:03:11.118052Z","iopub.status.idle":"2023-09-18T03:03:11.129201Z","shell.execute_reply.started":"2023-09-18T03:03:11.118008Z","shell.execute_reply":"2023-09-18T03:03:11.128072Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\nrequire([\"base/js/namespace\"],function(Jupyter) {\n    Jupyter.notebook.kernel.execute(\"config = get_ipython().config\");\n    Jupyter.notebook.kernel.execute(\"config.NotebookApp.iopub_msg_rate_limit = 4000.0\");\n});\n"},"metadata":{}}]},{"cell_type":"code","source":"# model architecture for training\n\n# Define the Squeeze-and-Excitation block\nclass SEBlock(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_channels // reduction, in_channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n# Modify the ResNetBlock to include the SE block\nclass ResNetBlock(nn.Module):\n    def __init__(self, in_channels, num_filters, kernel_size=3):\n        super(ResNetBlock, self).__init__()\n        self.resnet_block = torch.nn.Sequential(\n            *[\n                nn.Conv2d(num_filters, num_filters, kernel_size, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(num_filters, num_filters, kernel_size, padding=1),\n            ]\n        )\n        self.se_block = SEBlock(num_filters)  # Add SE block here\n        self.input = nn.Sequential()\n\n    def forward(self, x):\n        inp = self.input(x)\n        x = self.resnet_block(x)\n        x = self.se_block(x)  # Apply SE block\n        return x + inp\n\n\nclass Model(nn.Module):\n    def __init__(\n        self, in_channels=4, factor=2, scale=3, num_of_residual_blocks=20, num_filters=64, kernel_size=3, **kwargs\n    ):\n        super().__init__()\n        self.num_of_residual_blocks = num_of_residual_blocks\n        self.scale = scale\n        self.factor = factor\n        self.in_channels = in_channels\n        self.num_filters = num_filters\n        self.kernel_size = kernel_size\n        self.res_blocks = nn.Sequential(\n            *[\n                ResNetBlock(\n                    in_channels=in_channels,\n                    num_filters=num_filters,\n                    kernel_size=kernel_size,\n                )\n            ]\n            * num_of_residual_blocks\n        )\n\n        self.upsample = nn.Sequential(\n            *[\n                nn.Conv2d(num_filters, num_filters * (factor**2), kernel_size=kernel_size, padding=1, **kwargs),\n                nn.PixelShuffle(upscale_factor=factor),\n            ]\n            * scale\n        )\n        self.resnet_input = nn.Conv2d(in_channels, num_filters, kernel_size=1)\n        self.output_layer = nn.Conv2d(num_filters, in_channels, kernel_size=3, padding=1)\n        self.resnet_out = nn.Conv2d(self.num_filters, self.num_filters, kernel_size=kernel_size, padding=1)\n\n    def forward(self, x):\n        x = self.resnet_input(x)\n        x_res = self.res_blocks(x)\n        x_res = self.resnet_out(x_res)\n        out = x + x_res\n        out = self.upsample(out)\n        return self.output_layer(out)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:11.134484Z","iopub.execute_input":"2023-09-18T03:03:11.134882Z","iopub.status.idle":"2023-09-18T03:03:11.157423Z","shell.execute_reply.started":"2023-09-18T03:03:11.134850Z","shell.execute_reply":"2023-09-18T03:03:11.156294Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# add lightning module for pytorch\n\nclass FlowFieldModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        self.model = Model()\n        # Loss function\n        self.loss_fn = nn.MSELoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        inputs, targets = batch\n        outputs = self(inputs)\n        loss = self.loss_fn(outputs, targets)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        inputs, targets = batch\n        outputs = self(inputs)\n        val_loss = self.loss_fn(outputs, targets)\n        self.log(\"val_loss\", val_loss, sync_dist=True)\n\n    def configure_optimizers(self):\n        optimizer = Adam(params=self.parameters(), lr=0.001)\n        scheduler = ReduceLROnPlateau(optimizer=optimizer)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss'\n            }\n        }","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:11.159878Z","iopub.execute_input":"2023-09-18T03:03:11.160628Z","iopub.status.idle":"2023-09-18T03:03:11.171851Z","shell.execute_reply.started":"2023-09-18T03:03:11.160569Z","shell.execute_reply":"2023-09-18T03:03:11.170698Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# load and preprocess custom utilized dataset\n\nclass FlowFieldDataset(Dataset):\n    def __init__(self, input_path, mode):\n        assert mode in [\"train\", \"val\", \"test\"]\n        self.mode = mode\n        self.csv_file = pd.read_csv(input_path + f\"{mode}.csv\")\n        if mode == \"test\":\n            self.csv_file = pd.read_csv(input_path + f\"{mode}.csv\")\n        self.LR_path = input_path + \"flowfields/LR/\" + mode\n        self.HR_path = input_path + \"flowfields/HR/\" + mode\n\n        self.mean = np.array([0.24, 28.0, 28.0, 28.0])\n        self.std = np.array([0.068, 48.0, 48.0, 48.0])\n\n    def transform(self, x):\n        return Compose([ToTensor(), Normalize(self.mean, self.std, inplace=True)])(x)\n\n    def __len__(self):\n        return len(self.csv_file)\n\n    def __getitem__(self, idx):\n        # input\n        if self.mode == \"test\":\n            id = self.csv_file[\"id\"][idx]\n            rho_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"rho_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n            ux_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"ux_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n            uy_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"uy_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n            uz_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"uz_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n            X = np.stack([rho_i, ux_i, uy_i, uz_i], axis=2)\n            return id, self.transform(X)\n\n        rho_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"rho_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n        ux_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"ux_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n        uy_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"uy_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n        uz_i = np.fromfile(self.LR_path + \"/\" + self.csv_file[\"uz_filename\"][idx], dtype=\"<f4\").reshape(16, 16)\n        # output\n        rho_o = np.fromfile(self.HR_path + \"/\" + self.csv_file[\"rho_filename\"][idx], dtype=\"<f4\").reshape(128, 128)\n        ux_o = np.fromfile(self.HR_path + \"/\" + self.csv_file[\"ux_filename\"][idx], dtype=\"<f4\").reshape(128, 128)\n        uy_o = np.fromfile(self.HR_path + \"/\" + self.csv_file[\"uy_filename\"][idx], dtype=\"<f4\").reshape(128, 128)\n        uz_o = np.fromfile(self.HR_path + \"/\" + self.csv_file[\"uz_filename\"][idx], dtype=\"<f4\").reshape(128, 128)\n        X = np.stack([rho_i, ux_i, uy_i, uz_i], axis=2)\n        Y = np.stack([rho_o, ux_o, uy_o, uz_o], axis=2)\n        return self.transform(X), self.transform(Y)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:11.173848Z","iopub.execute_input":"2023-09-18T03:03:11.174449Z","iopub.status.idle":"2023-09-18T03:03:11.197763Z","shell.execute_reply.started":"2023-09-18T03:03:11.174413Z","shell.execute_reply":"2023-09-18T03:03:11.196156Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Set hyperparameters here\n    num_epochs = 300\n    learning_rate = 1e-3\n    batch_size = 32\n    train_dataset = FlowFieldDataset(input_path=input_path, mode=\"train\")\n    val_dataset = FlowFieldDataset(input_path=input_path, mode=\"val\")\n    test_dataset = FlowFieldDataset(input_path=input_path, mode=\"test\")\n\n    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n    val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n    test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, pin_memory=False)\n\n    model =FlowFieldModel()\n\n    # Define a logger for TensorBoard\n    logger = TensorBoardLogger(\"logs\", name=\"tensorboard\")\n\n    # Define a ModelCheckpoint callback to save the best model\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=checkpoint_dir,\n        filename=\"ckpt\",\n        save_top_k=1,\n        monitor=\"val_loss\",\n        mode=\"min\",\n    )\n\n    trainer = Trainer(\n        deterministic=True,\n        max_epochs=num_epochs,\n        devices=2,\n        accelerator=\"gpu\",\n         # Set to 0 for CPU or specify GPU device IDs\n        logger=logger,\n        log_every_n_steps=5,\n        callbacks=[checkpoint_callback],\n    )\n\n\n    trainer.fit(model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:03:11.201989Z","iopub.execute_input":"2023-09-18T03:03:11.202816Z","iopub.status.idle":"2023-09-18T03:36:09.353557Z","shell.execute_reply.started":"2023-09-18T03:03:11.202784Z","shell.execute_reply":"2023-09-18T03:36:09.351775Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# perform the inference for the trained model\n\nprogress_bar = tqdm(range(len(test_dataloader)))\npredictions = {}\nids = []\nfor idx, batch in enumerate(test_dataloader):\n    id, inputs = batch\n    outputs = model(inputs)\n    outputs = outputs.permute(0, 2, 3, 1)\n    predictions[idx] = outputs.cpu().detach().numpy().flatten(order=\"C\").astype(np.float32)\n    ids.append(id.cpu().detach().numpy()[0])\n    progress_bar.set_description(f\"test prediction: {idx}\")\n    progress_bar.update(1)\nprogress_bar.close()\n\ndf = pd.DataFrame.from_dict(predictions).T\ndf[\"id\"] = ids\n# move id to first column\ncols = df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndf = df[cols]\n# reset index\ndf = df.reset_index(drop=True)\n\n# Get the current date and time\ncurrent_datetime = datetime.datetime.now()\n\n# Format the date and time as a string\nformatted_datetime = current_datetime.strftime(\"%Y%m%d%H%M%S\")\n\n# Define the filename with the formatted date and time\nfilename = f\"predictions_{formatted_datetime}.csv\"\n\n# Save the DataFrame to the CSV file\ndf.to_csv(filename, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:36:09.356653Z","iopub.execute_input":"2023-09-18T03:36:09.357075Z","iopub.status.idle":"2023-09-18T03:37:06.347396Z","shell.execute_reply.started":"2023-09-18T03:36:09.357036Z","shell.execute_reply":"2023-09-18T03:37:06.346333Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"test prediction: 172: 100%|██████████| 173/173 [00:13<00:00, 12.58it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# save the model weights\n\ntorch.save(model.state_dict(), \"model_weights0.0087.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:37:06.348999Z","iopub.execute_input":"2023-09-18T03:37:06.349861Z","iopub.status.idle":"2023-09-18T03:37:06.381573Z","shell.execute_reply.started":"2023-09-18T03:37:06.349824Z","shell.execute_reply":"2023-09-18T03:37:06.380581Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# save the full model including the weights\n\ntorch.save(model,\"model_full0.0087.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:37:06.383150Z","iopub.execute_input":"2023-09-18T03:37:06.383560Z","iopub.status.idle":"2023-09-18T03:37:06.395579Z","shell.execute_reply.started":"2023-09-18T03:37:06.383524Z","shell.execute_reply":"2023-09-18T03:37:06.394648Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# load the model weights\n\nmodel.load_state_dict(torch.load('model_weights0.0087.pth'))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:37:06.397174Z","iopub.execute_input":"2023-09-18T03:37:06.397764Z","iopub.status.idle":"2023-09-18T03:37:06.421999Z","shell.execute_reply.started":"2023-09-18T03:37:06.397731Z","shell.execute_reply":"2023-09-18T03:37:06.420933Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# load the full model\n\nmodel = torch.load('model_full0.0087.pth')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:37:06.425904Z","iopub.execute_input":"2023-09-18T03:37:06.426190Z","iopub.status.idle":"2023-09-18T03:37:06.434350Z","shell.execute_reply.started":"2023-09-18T03:37:06.426165Z","shell.execute_reply":"2023-09-18T03:37:06.433416Z"},"trusted":true},"execution_count":15,"outputs":[]}]}